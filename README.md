# ğŸš€ Azure Data Engineering Projects Portfolio

...


# ğŸ¥‡ Project 1: End-to-End Ingestion (ADLS â†’ ADF â†’ Azure SQL)

### ğŸ“Œ Overview  
Built a complete Azure Data Factory pipeline to ingest a **pipe-delimited text file** from **Azure Data Lake Storage Gen2** into **Azure SQL Database**.

### ğŸ— Architecture  
**ADLS (TXT)** â†’ **Azure Data Factory** â†’ **Azure SQL Database**

### ğŸ¯ Key Features
- âœ… Cloud file ingestion from ADLS  
- âœ… Azure Data Factory Copy activity  
- âœ… Schema mapping & type handling  
- âœ… Structured relational storage in Azure SQL  

### ğŸ“ˆ Learning Outcomes
- Built a full cloud ETL pipeline  
- Practiced ADLS â†’ ADF â†’ Azure SQL workflow  
- Designed and validated SQL tables  

ğŸ“ **Screenshots:**  
ğŸ‘‰ `pipelines/project-01/`

---

# ğŸ¥ˆ Project 2: Conditional Ingestion Pipeline (SQL â†’ ADLS)

### ğŸ“Œ Overview  
Designed a **control-flow driven Azure Data Factory pipeline** that loads data from **Azure SQL to Azure Data Lake only when the source contains more than 500 records.**

### ğŸ— Architecture  
**Azure SQL** â†’ **ADF (Lookup + If Condition)** â†’ **ADLS**

### ğŸ¯ Key Features
- âœ… Lookup activity to fetch source record count  
- âœ… If Condition for dynamic pipeline control  
- âœ… Conditional SQL â†’ ADLS ingestion  
- âœ… Prevents unnecessary data loads  

### ğŸ“ˆ Learning Outcomes
- Implemented pre-ingestion validation logic  
- Built conditional & decision-based pipelines  
- Applied real-world orchestration patterns  

ğŸ“ **Screenshots:**  
ğŸ‘‰ `pipelines/project2_conditional_ingestion/`

---

# ğŸ¥‰ Project 3: Metadata-Driven Multi-Table Ingestion Framework (SQL â†’ ADLS)

### ğŸ“Œ Overview  
Built a **single reusable Azure Data Factory pipeline** to ingest **multiple SQL tables into Azure Data Lake** using a **metadata-driven approach**, supporting both **sequential and batch execution modes.**

### ğŸ— Architecture  
**Azure SQL (Control Table)** â†’ **ADF (Lookup + ForEach + Parameterized Copy)** â†’ **ADLS**

### ğŸ¯ Key Features
- âœ… Metadata-driven ingestion using a **control/config table**  
- âœ… **Single pipeline** to ingest unlimited tables  
- âœ… **ForEach orchestration** for multi-table loading  
- âœ… Supports **sequential and parallel batch execution**  
- âœ… Fully **parameterized datasets and pipelines**  
- âœ… Automatic folder & file creation in ADLS  

### ğŸ“ˆ Learning Outcomes
- Designed a **scalable ingestion framework**  
- Implemented **Lookup + ForEach + dynamic expressions**  
- Built **enterprise-style reusable pipelines**  
- Practiced **batch vs sequential orchestration patterns**  

ğŸ“ **Screenshots:**  
ğŸ‘‰ `pipelines/project-03-mutilple_table_ingestion/`


# ğŸš€ Project 3: SCD Type 2 Implementation â€” Azure Data Factory

## ğŸ“Œ Overview
This project implements **Slowly Changing Dimension (Type 2)** using **Azure Data Factory Mapping Data Flow** to maintain **historical data changes** inside a dimension table.

The pipeline automatically:
- Detects new records
- Detects changed records
- Expires old versions
- Inserts new versions
- Preserves full history

This design mirrors **real-world enterprise data warehouse patterns** used in analytics and reporting systems.

---

## ğŸ— Architecture

<img src="images/scd2/pipeline.png" width="100%"/>

### Data Flow

```
Source â†’ Lookup â†’ Derived Column (Hash) â†’ Conditional Split
       â†’ Alter Row (Update Old)
       â†’ Alter Row (Insert New)
       â†’ Sink (Dimension Table)
```

---

## âš™ï¸ Tech Stack

- Azure Data Factory (Mapping Data Flow)
- Azure SQL Database
- SQL
- Dimensional Modeling

---

## ğŸ“‚ Project Structure

```
images/
 â””â”€â”€ scd2/
      â””â”€â”€ pipeline.png

sql/
 â””â”€â”€ create_tables.sql

README.md
```

---

## ğŸ—„ Database Design

### Staging Table
Stores latest snapshot of source data.

```sql
stg_customer
```

### Dimension Table (SCD2)

```sql
dim_customer
```

### Columns

| Column | Purpose |
|--------|----------|
| surrogate_key | Unique row identifier |
| customer_id | Business key |
| name, city, email | Attributes |
| start_date | Record start date |
| end_date | Record expiry date |
| is_current | Active flag |

---

## ğŸ”„ Data Flow Logic (Step-by-Step)

### 1ï¸âƒ£ Source
Reads latest data from staging table.

---

### 2ï¸âƒ£ Lookup
Matches with current records in dimension table.

Condition:
```
customer_id == dim_customer.customer_id
AND is_current == 1
```

Purpose: Identify existing active records.

---

### 3ï¸âƒ£ Derived Column (Hash)
Creates hashes for change detection.

Example:
```
hash_new = sha1(name + city + email)
hash_old = sha1(dim_name + dim_city + dim_email)
```

Purpose: Efficient comparison of records.

---

### 4ï¸âƒ£ Conditional Split

Routes rows into:

| Type | Condition |
|--------|-----------|
| NEW | No existing match |
| CHANGED | Hash mismatch |
| SAME | Hash match |

---

### 5ï¸âƒ£ Alter Row â€” Expire Old Records
For **changed rows**:

```
updateIf(true())
```

Updates:
```
end_date = currentUTC()
is_current = 0
```

---

### 6ï¸âƒ£ Alter Row â€” Insert New Records
For **new + changed rows**:

```
insertIf(true())
```

Sets:
```
start_date = currentUTC()
end_date = '9999-12-31'
is_current = 1
```

---

### 7ï¸âƒ£ Sink
Loads results into dimension table with:
- Insert enabled
- Update enabled

---

## ğŸ“Š Example Result

### Initial Load

| id | city |
|----|-----------|
| 1 | Hyderabad |

### After Update

| sk | id | city | start | end | current |
|----|----|-----------|--------|--------|---------|
| 1 | 1 | Hyderabad | 2025 | 2026 | 0 |
| 2 | 1 | Bangalore | 2026 | 9999 | 1 |

History is preserved.

---

## ğŸ¯ Key Features

- Pure Mapping Data Flow implementation
- No stored procedures
- No incremental logic
- Lookup-based matching
- Hash-based change detection
- Conditional routing
- Expire + Insert pattern
- Historical tracking

---

## ğŸ“ˆ Learning Outcomes

- Implemented SCD Type 2 from scratch
- Built Lookup + Conditional Split + Alter Row transformations
- Designed dimension tables with history tracking
- Practiced enterprise ETL patterns
- Worked with real-world Azure Data Factory workflows

---

## â–¶ï¸ How to Run

1. Create tables in Azure SQL
2. Import datasets into ADF
3. Deploy Mapping Data Flow
4. Run pipeline
5. Validate `dim_customer` history

---

## ğŸ“· Screenshots

Place screenshots inside:

```
images/scd2/
```

Example:
```
pipeline.png
```

---

# â­ Summary

This project demonstrates a **production-style SCD Type 2 implementation** using Azure Data Factory, enabling reliable historical tracking for analytics and reporting workloads.





